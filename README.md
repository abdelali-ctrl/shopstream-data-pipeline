<p align="center">
  <img src="https://img.shields.io/badge/PostgreSQL-316192?style=for-the-badge&logo=postgresql&logoColor=white" alt="PostgreSQL"/>
  <img src="https://img.shields.io/badge/Amazon_S3-569A31?style=for-the-badge&logo=amazons3&logoColor=white" alt="AWS S3"/>
  <img src="https://img.shields.io/badge/Snowflake-29B5E8?style=for-the-badge&logo=snowflake&logoColor=white" alt="Snowflake"/>
  <img src="https://img.shields.io/badge/dbt-FF694B?style=for-the-badge&logo=dbt&logoColor=white" alt="dbt"/>
  <img src="https://img.shields.io/badge/Apache_Airflow-017CEE?style=for-the-badge&logo=apacheairflow&logoColor=white" alt="Airflow"/>
  <img src="https://img.shields.io/badge/Power_BI-F2C811?style=for-the-badge&logo=powerbi&logoColor=black" alt="Power BI"/>
  <img src="https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white" alt="Docker"/>
</p>

# ğŸ›’ ShopStream - Modern Cloud Data Pipeline

A **production-ready data engineering project** demonstrating a complete end-to-end cloud data pipeline for e-commerce analytics. Built with the Modern Data Stack.

---

## ğŸ“Š Architecture Overview

```mermaid
flowchart LR
    subgraph Source["ğŸ—„ï¸ Source Layer"]
        A[("PostgreSQL")]
    end
    
    subgraph Ingestion["ğŸ“¤ Ingestion Layer"]
        B["Python ETL Scripts"]
    end
    
    subgraph Storage["â˜ï¸ Cloud Storage"]
        C[("AWS S3\nData Lake")]
    end
    
    subgraph Warehouse["â„ï¸ Data Warehouse"]
        D[("Snowflake")]
    end
    
    subgraph Transform["ğŸ”„ Transformation"]
        E["dbt\n(Staging â†’ Core â†’ Marts)"]
    end
    
    subgraph Orchestration["âš™ï¸ Orchestration"]
        F["Apache Airflow"]
    end
    
    subgraph BI["ğŸ“ˆ Business Intelligence"]
        G["Power BI Dashboard"]
    end
    
    A --> B --> C --> D --> E
    F -.->|orchestrates| B
    F -.->|orchestrates| E
    E --> G
```

---

## âœ¨ Features

- ğŸ”„ **Automated Data Generation** â€” Realistic e-commerce data using Faker
- â˜ï¸ **Cloud-Native Architecture** â€” AWS S3 data lake with partitioned storage
- â„ï¸ **Snowflake Data Warehouse** â€” Scalable cloud DWH with staging tables
- ğŸ“ **Dimensional Modeling** â€” Star schema with dbt (dimensions, facts, marts)
- ğŸ“Š **Business Intelligence** â€” Power BI dashboard for sales analytics
- âš™ï¸ **Orchestration** â€” Apache Airflow DAG for automated pipelines
- ğŸ³ **Docker Ready** â€” One-command setup with Docker Compose

---

## ğŸ“Š Power BI Dashboard

### Vue d'ensemble (Overview)
![Vue d'ensemble](docs/Vue%20d'ensemble.png)

### Analyse Clients (Customer Analysis)
![Analyse Clients](docs/Analyse%20Clients.png)

## ğŸš€ Quick Start

### Option 1: Docker (Recommended)

```bash
# Clone the repository
git clone https://github.com/yourusername/shopstream.git
cd shopstream

# Start all services
docker-compose up -d

# Access Airflow UI
open http://localhost:8080  # admin/admin
```

### Option 2: Manual Setup

```bash
# 1. Create Python virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# 2. Configure environment
cp .env.example .env
# Edit .env with your credentials

# 3. Create PostgreSQL database and tables
psql -U postgres -c "CREATE DATABASE shopstream;"
psql -U postgres -d shopstream -f scripts/schema.sql

# 4. Generate sample data
python scripts/generate_data.py

# 5. Export to S3
python scripts/export_to_s3.py

# 6. Run dbt transformations
cd dbt_part/shopstream_dbt
dbt run
```

---

## ğŸ—ï¸ Project Structure

```
shopstream/
â”‚
â”œâ”€â”€ ğŸ“œ scripts/                         # Python & SQL Scripts
â”‚   â”œâ”€â”€ schema.sql                      # PostgreSQL schema (6 tables)
â”‚   â”œâ”€â”€ generate_data.py                # Generate sample e-commerce data
â”‚   â”œâ”€â”€ export_to_s3.py                 # Export PostgreSQL â†’ S3
â”‚   â”œâ”€â”€ snowflake_setup.sql             # Complete Snowflake setup (DB, schemas, tables)
â”‚   â”œâ”€â”€ snowflake_copy_into.sql         # Snowflake COPY INTO commands
â”‚   â””â”€â”€ snowflake_verify_data.sql       # Data verification queries
â”‚
â”œâ”€â”€ ğŸ”„ dbt_part/                        # dbt Transformation Project
â”‚   â””â”€â”€ shopstream_dbt/
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ staging/                # stg_orders
â”‚       â”‚   â”œâ”€â”€ core/                   # dim_customers, dim_products, fact_orders
â”‚       â”‚   â””â”€â”€ marts/                  # mart_sales_overview, mart_customer_ltv, mart_product_performance
â”‚       â””â”€â”€ dbt_project.yml
â”‚
â”œâ”€â”€ âš™ï¸ airflow/                         # Airflow Orchestration
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ shopstream_pipeline_dag.py  # Daily ETL pipeline DAG
â”‚   â””â”€â”€ airflow.cfg
â”‚
â”œâ”€â”€ ğŸ“Š powerbi/                         # Business Intelligence
â”‚   â””â”€â”€ ShopStream_Dashboard.pbix
â”‚
â”œâ”€â”€ ğŸ“ docs/                            # Documentation
â”‚   â””â”€â”€ dashboard_preview.md            # Dashboard screenshots guide
â”‚
â”œâ”€â”€ ğŸ³ docker-compose.yml               # Docker setup (PostgreSQL + Airflow)
â”œâ”€â”€ .env.example                        # Environment template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“ˆ Data Model

### Star Schema

```mermaid
erDiagram
    FACT_ORDERS ||--o{ DIM_CUSTOMERS : "customer_key"
    FACT_ORDERS ||--o{ DIM_PRODUCTS : "product_key"
    
    DIM_CUSTOMERS {
        int customer_key PK
        string email
        string full_name
        string country
        string customer_segment
    }
    
    DIM_PRODUCTS {
        int product_key PK
        string name
        string category
        decimal price
    }
    
    FACT_ORDERS {
        int order_key PK
        int customer_key FK
        int product_key FK
        date date_key
        int quantity_sold
        decimal line_revenue
    }
```

### Data Marts

| Mart | Description |
|------|-------------|
| `mart_sales_overview` | Daily sales by country, category, segment |
| `mart_customer_ltv` | Customer RFM segmentation & churn risk |
| `mart_product_performance` | ABC analysis & product rankings |

---

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description |
|----------|-------------|
| `POSTGRES_HOST` | PostgreSQL host |
| `POSTGRES_PASSWORD` | Database password |
| `AWS_S3_BUCKET` | S3 bucket name |
| `AWS_REGION` | AWS region |

See [.env.example](.env.example) for all variables.

---

## ğŸ“ Skills Demonstrated

- âœ… End-to-end data pipeline design
- âœ… Cloud infrastructure (AWS S3, Snowflake)
- âœ… Dimensional modeling (Kimball methodology)
- âœ… Modern data stack (dbt, Airflow)
- âœ… Docker containerization
- âœ… Python ETL development
- âœ… Business Intelligence

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ‘¨â€ğŸ’» Author

**Abdelali** - Data Engineering Student @ UEMF

---

<p align="center">
  <i>Built with â¤ï¸ as part of the Cloud Data Engineering curriculum</i>
</p>
